import csv
import logging
import pandas as pd
from jobspy import scrape_jobs

# ğŸ”¹ Setup logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# ğŸ”¹ Scrape jobs from Indeed & LinkedIn
jobs = scrape_jobs(
    site_name=["indeed", "linkedin"],  # âœ… Still scraping both sites
    search_term="software engineer",
    location="San Francisco, CA",
    pages=25,  # âœ… Increased pages for deeper scraping
    max_results=500,  # âœ… Increased max job limit
    date_posted="last_24_hours"
)

# ğŸ”¹ Check if we got results
if not jobs.empty:
    logging.info(f"âœ… Scraped {len(jobs)} jobs from Indeed & LinkedIn.")

    # âœ… Keep only relevant fields
    valid_columns = ["site", "title", "company", "location", "description"]
    jobs = jobs[[col for col in valid_columns if col in jobs.columns]]

    # âœ… Remove duplicate jobs (same title, company, and location)
    jobs.drop_duplicates(subset=["title", "company", "location"], keep="first", inplace=True)

    # âœ… Clean text fields (e.g., remove HTML from descriptions)
    jobs["description"] = jobs["description"].str.replace(r"<.*?>", "", regex=True)

    # âœ… Fill missing values with empty strings
    jobs.fillna("", inplace=True)

    # âœ… Convert to list of dicts for CSV writing
    jobs_dicts = jobs.to_dict(orient="records")

    # âœ… Save to jobs.csv
    csv_filename = "jobs.csv"
    with open(csv_filename, "w", newline="", encoding="utf-8") as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=jobs.columns)
        dict_writer.writeheader()
        dict_writer.writerows(jobs_dicts)

    logging.info(f"ğŸ“‚ Jobs saved to {csv_filename}")

else:
    logging.warning("âš ï¸ No jobs found. Try adjusting search parameters.")
